{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ef3d1a",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Bluewalm Module\n",
    "<br>\n",
    "\n",
    "## LLM Notebook\n",
    "<br>\n",
    "\n",
    "This notebook is meant to illustrate the basic usage of the **bluewalm** pytorch extension module. \n",
    "\n",
    "First, let's import the modules we need. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c6bc42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# now we import is our module\n",
    "import bluewalm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8ab17",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Attention Layer\n",
    "<br>\n",
    "In the cell below we create a softmax attention layer. <br>\n",
    "It's using the usual scaled dot-product attention with causal masking. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fe04e5-3284-4357-8792-4dac75b3d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayerSM(torch.nn.Module):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        assert dim % n_heads == 0\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.wq = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.wk = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.wv = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.wo = torch.nn.Linear(dim, dim, bias=False)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.wq.reset_parameters()\n",
    "        self.wk.reset_parameters()\n",
    "        self.wv.reset_parameters()\n",
    "        self.wo.reset_parameters()\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                k_cache: torch.Tensor,\n",
    "                v_cache: torch.Tensor):\n",
    "\n",
    "        # x : (bsz, seqlen, dim)\n",
    "        # k_cache : (bsz, cache_len, dim)\n",
    "        # v_cache : (bsz, cache_len, dim)\n",
    "        \n",
    "        bsz, seqlen, _ = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        \n",
    "        k = torch.cat((k_cache, k), dim=1)\n",
    "        v = torch.cat((v_cache, v), dim=1)\n",
    "        \n",
    "        q = q.view(bsz, -1, self.n_heads, self.head_dim)\n",
    "        k = k.view(bsz, -1, self.n_heads, self.head_dim)\n",
    "        v = v.view(bsz, -1, self.n_heads, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1, 2)\n",
    "        # q : (bsz, n_heads, seqlen, head_dim)\n",
    "        \n",
    "        k = k.transpose(1, 2)\n",
    "        # k : (bsz, n_heads, cache_len + seqlen, head_dim)\n",
    "        \n",
    "        v = v.transpose(1, 2)\n",
    "        # v : (bsz, n_heads, cache_len + seqlen, head_dim)\n",
    "        \n",
    "        scores = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        # scores : (bsz, n_heads, seqlen, head_dim)\n",
    "        \n",
    "        scores = scores.transpose(1, 2).contiguous().view(bsz, seqlen, dim)\n",
    "        # scores : (bsz, seqlen, dim)\n",
    "\n",
    "        output = self.wo(scores)\n",
    "        # output : (bsz, seqlen, dim)\n",
    "\n",
    "        k = k.transpose(1, 2).view(bsz, -1, dim)\n",
    "        # k : (bsz, cache_len + seqlen, dim)\n",
    "        \n",
    "        v = v.transpose(1, 2).view(bsz, -1, dim)\n",
    "        # v : (bsz, cache_len + seqlen, dim)\n",
    "        \n",
    "        return output, k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b0a04-d4f4-47df-be76-6e5afee62ac6",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the cell below we create a softplus attention layer. <br>\n",
    "It's using the softplus attention. <br>\n",
    "This is the new stuff. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71d664b-ddde-40ab-a52b-4f037a48ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import what we need for the attention layer\n",
    "from bluewalm.softplus_attention import attention_operator\n",
    "from bluewalm.softplus_attention import QueryProjection, KeyProjection, ValueProjection, OutProjection\n",
    "\n",
    "\n",
    "class AttentionLayerSP(torch.nn.Module):\n",
    "    def __init__(self, dim, core_dim):\n",
    "        super().__init__()\n",
    "        self.wq = QueryProjection(dim, core_dim)\n",
    "        self.wk = KeyProjection(dim, core_dim)\n",
    "        self.wv = ValueProjection(dim, core_dim)\n",
    "        self.wo = OutProjection(dim, core_dim)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.wq.reset_parameters()\n",
    "        self.wk.reset_parameters()\n",
    "        self.wv.reset_parameters()\n",
    "        self.wo.reset_parameters()\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                k_cache: torch.Tensor,\n",
    "                v_cache: torch.Tensor):\n",
    "        \n",
    "        # x : (bsz, seqlen, dim)\n",
    "        # k_cache : (bsz, core_dim, cache_len)\n",
    "        # v_cache : (bsz, core_dim, cache_len)\n",
    "        \n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        # q : (bsz, seqlen, core_dim)\n",
    "        # k : (bsz, core_dim, seqlen)\n",
    "        # v : (bsz, core_dim, seqlen)\n",
    "        \n",
    "        # reuse attention keys and values by concatenating to the current ones \n",
    "        k = torch.cat((k_cache, k), dim=2)\n",
    "        v = torch.cat((v_cache, v), dim=2)\n",
    "        # k : (bsz, core_dim, cache_len + seqlen)\n",
    "        # v : (bsz, core_dim, cache_len + seqlen)\n",
    "        \n",
    "        # q, k and v must be contiguous here \n",
    "        scores = attention_operator(q, k, v)\n",
    "        # scores : (bsz, seqlen, core_dim)\n",
    "        \n",
    "        output = self.wo(scores)\n",
    "        # output : (bsz, seqlen, dim)\n",
    "        return output, k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726e4a2b-0fbf-4ad9-bea4-439a71b540d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "The secret sauce in the softplus attention layer is the softplus attention operator. <br>\n",
    "Let's see what the documentation says about it. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aacade2-038b-427e-926f-5ca191313843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attention operator. Realizes softplus attention. \n",
      "    Args: \n",
      "         q (torch.Tensor) : (b x s x r) dimensional tensor, the query tensor, \n",
      "         k (torch.Tensor) : (b x r x t) dimensional tensor, the key tensor, \n",
      "         v (torch.Tensor) : (b x r x t) dimensional tensor, the value tensor, \n",
      "        \n",
      "        where\n",
      "             b : batch size\n",
      "             s : query length\n",
      "             t : key-value length\n",
      "             r : core dimension\n",
      "    \n",
      "        The three tensors must be on the same device and they must be stored in the same floating point format. \n",
      "        Supported formats : torch.float32, torch.float16, torch.bfloat16. \n",
      "        \n",
      "        Returns: \n",
      "        qkv (torch.Tensor) : (b x s x r) dimensional tensor, \n",
      "                             stored on the same device as the input tensors, \n",
      "                             and in the same floating point format. \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(attention_operator.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18790f21",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "The astute reader will notice, that the number of attention heads is gone. <br>\n",
    "Instead of that, the softplus attention depends on an 'r' hyperparameter, which we call the \"core dimension\". <br>\n",
    "<br>\n",
    "Technically, the training procedure will select the number of attention heads, as it does not have to be an integer anymore. <br>\n",
    "The core dimension is a hyperparameter, but simple experiments suggest, that it might be good to set it to 4x the token dimension. <br>\n",
    "We can also use a **heuristic** designed to estimate good core dimension values. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ced369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424\n"
     ]
    }
   ],
   "source": [
    "from bluewalm.softplus_attention import heuristic_core_dim\n",
    "\n",
    "dim = 128\n",
    "core_dim = heuristic_core_dim(dim)\n",
    "print(core_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a09a6-7a4c-46ef-904a-95c096c5b624",
   "metadata": {},
   "source": [
    "Let's see what the documentation says about it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57ad97e-3ac8-4aa1-b6ff-888562b75495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " takes dim and returns a friendly suggestion for core dim \n"
     ]
    }
   ],
   "source": [
    "print(heuristic_core_dim.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37350d9d-566a-48b6-b55d-c1516f6ae607",
   "metadata": {},
   "source": [
    "<br>\n",
    "According to simple experiments we performed, accuracy grows fast in the token dimension when the core dimension is set with the above heuristic. <br> This should be taken with a pinch of salt, since it's a heuristic. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742c0af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br><br>\n",
    "Softplus attention layers were meant to be used on the **GPU**. <br>\n",
    "However, they run on the **CPU** as well, and can be adjusted to and implemented for **any AI accelerator**. <br>\n",
    "By the way, they are **FSDP-ready**. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f99b1-fc0d-4987-9295-bfbddec83052",
   "metadata": {},
   "source": [
    "<br>\n",
    "Below, we create one instance of the softmax attention layer and one instance of the softplus attention layer. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52427846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionLayerSM(\n",
      "  (wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (wk): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "  (wo): Linear(in_features=128, out_features=128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "n_heads = 4\n",
    "softmax_attention = AttentionLayerSM(dim, n_heads)\n",
    "print(softmax_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de5a0e9-3603-4ed8-92fa-4a3687f97831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionLayerSP(\n",
      "  (wq): QueryProjection(dim=128, core_dim=128, precision=fp32, size=0.0625 MB)\n",
      "  (wk): KeyProjection(dim=128, core_dim=128, precision=fp32, size=0.0625 MB)\n",
      "  (wv): ValueProjection(dim=128, core_dim=128, precision=fp32, size=0.0625 MB)\n",
      "  (wo): OutProjection(dim=128, core_dim=128, precision=fp32, size=0.0625 MB)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "core_dim = 128\n",
    "softplus_attention = AttentionLayerSP(dim, core_dim)\n",
    "print(softplus_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6297f00-240e-4779-8e53-de4653a88eb8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Positionwise Feedforward Layer (FFN)\n",
    "<br>\n",
    "Below is a simple implementation of the positionwise feedforward layer to be found in LLM architectures. <br>\n",
    "It's a gated linear unit, where \"hidden_dim\" is the usual bottleneck dimension. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e826a9-a5b7-4a8e-be6d-92ddda1ba5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.w1 = torch.nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = torch.nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = torch.nn.Linear(dim, hidden_dim, bias=False)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.w1.reset_parameters()\n",
    "        self.w2.reset_parameters()\n",
    "        self.w3.reset_parameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w2(torch.nn.functional.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b3642-e8a5-4ee0-95b3-a0bf40a8cae4",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's create an instance of this class. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e931b3ca-4358-492c-962f-e853bb5995c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (w1): Linear(in_features=128, out_features=256, bias=False)\n",
      "  (w2): Linear(in_features=256, out_features=128, bias=False)\n",
      "  (w3): Linear(in_features=128, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "combinator_sm = FeedForward(128, 256)\n",
    "print(combinator_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f65808-2e19-4b43-8e92-cf266430c965",
   "metadata": {},
   "source": [
    "<br>\n",
    "Below we import the replacement layer from the bluewalm module. <br>\n",
    "This layer was meant to replace positionwise feedforward layers. <br>\n",
    "Using this layer is expected to result in a better (accuracy / compute) ratio. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f08791fc-31c4-4170-9c15-dea36cd44c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinator layer for softplus attention. Changes the internal representation of the tokens within the forward \n",
      "     pass of the Transformer layer. It was meant to be used as a replacement for positionwise feedforward layers. \n",
      "     The values are initialized from :math:`\\mathcal{U}(\\frac{-1}{\\sqrt{k}}, \\frac{1}{\\sqrt{k}})`, \n",
      "     where :math:`k = \\frac{shape[0] + shape[1]}{2}`. \n",
      "    \n",
      "    Args:\n",
      "        dim (int): The dimension of the layer. Should be the token dimension. \n",
      "    \n",
      "    KwArgs:\n",
      "        device (str): The device the layer is constructed on. \n",
      "                      By default, this is `cpu`. \n",
      "        dtype (torch.dtype): Sets the datatype of the layer after construction. \n",
      "                             By default, this is `torch.float32`. \n",
      "                             The following are supported: `torch.float32`, `torch.float16`, `torch.bfloat16`. \n",
      "    \n",
      "    Attributes:\n",
      "        weight (Tensor): the learnable weights of the module. \n",
      "        dim (int) : The dimension of the layer. Should be the token dimension. \n",
      "        shape (Tuple) : If this was a linear layer, then it would be a linear layer of this shape. \n",
      "                        This is used in initialization. By default, this is (dim, dim). \n",
      "    \n",
      "    Examples::\n",
      "        >>> Combinator = bluewalm.softplus_attention.Combinator\n",
      "        >>> combinator = Combinator(128, device='cuda')\n",
      "        >>> print(combinator)\n",
      "        Combinator(dim=128, precision=fp32, size=0.125 MB)\n",
      "        >>> combinator.normal_(mean=0.0, std=1.0)\n",
      "        >>> bsz, query_len, dim = 64, 1024, 128\n",
      "        >>> x = torch.zeros((bsz, query_len, dim), device='cuda').uniform_(-1.0, 1.0)\n",
      "        >>> output = combinator(x)\n",
      "        >>> print(output.shape)\n",
      "        torch.Size([64, 1024, 128])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from bluewalm.softplus_attention import Combinator\n",
    "\n",
    "\n",
    "print(Combinator.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec2a4a-7338-40ec-a30b-fa4eab546d14",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's create an instance of this class. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6eb500f-d44b-4073-ad30-5a7ae3c07068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinator(dim=128, precision=fp32, size=0.125 MB)\n"
     ]
    }
   ],
   "source": [
    "combinator_sp = Combinator(128)\n",
    "combinator_sp.uniform_(-1.0, 1.0)\n",
    "\n",
    "print(combinator_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cdb564-5f10-45a9-8a3f-5d5fe774d326",
   "metadata": {},
   "source": [
    "<br>\n",
    "Notice, that the bottleneck dimension is absent. <br>\n",
    "That's because there is no bottleneck in this layer. <br> \n",
    "<br>\n",
    "We also expect a loss of accuracy, when compared to the positionwise feedforward layer above. <br>\n",
    "<br>\n",
    "We have found that the accuracy advantage we can gain by using the GLU above might not be worth the cost. <br>\n",
    "We recommend improving accuracy by increasing the token dimension or the core dimension instead. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9bfd6-3bf5-401d-ac3c-1ae935d4fc38",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### benchmarking GPU memory\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381282d-c31f-4cdb-b5fc-a5c4df37119e",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now we are going to measure the GPU memory they use. <br>\n",
    "We are going to use a context manager for this. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72af93bb-5ac0-42bc-b5bb-f1f0e88cb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def benchmark_gpu_memory():\n",
    "    start = torch.cuda.max_memory_allocated()\n",
    "    outside_context = set(globals().keys())\n",
    "    error_msg = None\n",
    "    try:\n",
    "        yield\n",
    "    except torch.OutOfMemoryError:\n",
    "        start = None\n",
    "    finally:\n",
    "        end = torch.cuda.max_memory_allocated()\n",
    "        # delete the variables allocated within the context \n",
    "        inside_context = set(globals().keys()) - outside_context\n",
    "        for name in inside_context:\n",
    "            del globals()[name]\n",
    "        # reset peak memory stats\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        if start is not None:\n",
    "            memory_used = (end - start) / 1024**2\n",
    "            memory_used = math.ceil(memory_used)\n",
    "            memory_used = int(memory_used)\n",
    "            print(\"used\", \"{:06d}\".format(memory_used), \"MB of GPU memory\")\n",
    "        else:\n",
    "            print(\"out of memory\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b04c3",
   "metadata": {},
   "source": [
    "<br>\n",
    "We are going to use some convenience functions: \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8798bc4f-7439-4744-88cc-dd917ec603f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(bsz, seqlen, dim, device, dtype=torch.float32):\n",
    "    shape = (bsz, seqlen, dim)\n",
    "    x = torch.zeros(shape, device=device, dtype=dtype).uniform_(-1.0, 1.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_sm_cache(bsz, cache_len, dim, device, dtype=torch.float32):\n",
    "    shape = (bsz, cache_len, dim)\n",
    "    k_cache = torch.empty(shape, device=device, dtype=dtype).uniform_(-1.0, 1.0)\n",
    "    v_cache = torch.empty(shape, device=device, dtype=dtype).uniform_(-1.0, 1.0)\n",
    "    return k_cache, v_cache\n",
    "\n",
    "\n",
    "def get_sp_cache(bsz, cache_len, core_dim, device, dtype=torch.float32):\n",
    "    shape = (bsz, core_dim, cache_len)\n",
    "    k_cache = torch.empty(shape, device=device, dtype=dtype).uniform_(-1.0, 1.0)\n",
    "    v_cache = torch.empty(shape, device=device, dtype=dtype).uniform_(-1.0, 1.0)\n",
    "    return k_cache, v_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284969d-dd38-4f3d-9a7e-017aa0a069ed",
   "metadata": {},
   "source": [
    "<br>\n",
    "....and some more convenience functions: <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04dc2f9c-321c-489e-bf16-1b286e1ed553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_sm(bsz, seqlen, dim, n_heads, cache_len):\n",
    "    print(\"running a single forward pass and then a backward pass of softmax attention....\")\n",
    "    # create the attention layer\n",
    "    softmax_attention = AttentionLayerSM(dim, n_heads).cuda()\n",
    "    # create the input\n",
    "    x = get_input(bsz, seqlen, dim, 'cuda')\n",
    "    k_cache, v_cache = get_sm_cache(bsz, cache_len, dim, 'cuda')\n",
    "    # all inputs will require grads\n",
    "    x.requires_grad = True\n",
    "    k_cache.requires_grad = True\n",
    "    v_cache.requires_grad = True\n",
    "    # run a forward pass\n",
    "    output, new_k_cache, new_v_cache = softmax_attention(x, k_cache, v_cache)\n",
    "    # now a backward pass\n",
    "    output.sum().backward()\n",
    "    assert x.grad is not None\n",
    "\n",
    "\n",
    "def forward_backward_sp(bsz, seqlen, dim, core_dim, cache_len):\n",
    "    print(\"running a single forward pass and then a backward pass of softplus attention....\")\n",
    "    # create the attention layer\n",
    "    softplus_attention = AttentionLayerSP(dim, core_dim).cuda()\n",
    "    # create the input\n",
    "    x = get_input(bsz, seqlen, dim, 'cuda')\n",
    "    k_cache, v_cache = get_sp_cache(bsz, cache_len, core_dim, 'cuda')\n",
    "    # all inputs will require grads\n",
    "    x.requires_grad = True\n",
    "    k_cache.requires_grad = True\n",
    "    v_cache.requires_grad = True\n",
    "    # run a forward pass\n",
    "    output, new_k_cache, new_v_cache = softplus_attention(x, k_cache, v_cache)\n",
    "    # now a backward pass\n",
    "    output.sum().backward()\n",
    "    assert x.grad is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db8460-aac9-4b6b-81b9-59c7872371b8",
   "metadata": {},
   "source": [
    "<br>\n",
    "We will use a python context manager for measuring the amount of GPU memory used. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "684031a6-ce36-4d04-9549-61065ad51e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running a single forward pass and then a backward pass of softmax attention....\n",
      "used 013598 MB of GPU memory\n",
      "\n",
      "running a single forward pass and then a backward pass of softplus attention....\n",
      "used 006791 MB of GPU memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bsz, seqlen, dim, core_dim, n_heads, cache_len = 32, 8192, 512, 512, 128, 0\n",
    "\n",
    "\n",
    "with benchmark_gpu_memory():\n",
    "    forward_backward_sm(bsz, seqlen, dim, n_heads, cache_len)\n",
    "\n",
    "with benchmark_gpu_memory():\n",
    "    forward_backward_sp(bsz, seqlen, dim, core_dim, cache_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d1b79-20b4-4c31-8a8c-d39c587c8d67",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### deployment into TensorRT\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d6e7d-4ae9-454e-b164-8b73eb1d5b8b",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now we are going to deploy both the softmax and the softplus attention layers into TensorRT format and run a compute benchmark. <br>\n",
    "In production the neural network most likely is going to be deployed into TensorRT format, so these compute benchmarks really count. <br>\n",
    "First, let's import the modules we need. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "871f709d-96af-4c19-8301-45c60e48f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from ml_dtypes import bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343ae46-bdad-4faa-b087-68e4a04a99ae",
   "metadata": {},
   "source": [
    "<br>\n",
    "We are going to use some convenience functions. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf7bf0a9-9fb4-408a-a1d2-47e5ccc82977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(command):\n",
    "    ''' \n",
    "        execute command; capture and print stdout\n",
    "        return stdout \n",
    "    ''' \n",
    "    # free up some memory \n",
    "    torch.cuda.empty_cache()\n",
    "    # execute command \n",
    "    command = command.split()\n",
    "    outputs = []\n",
    "    stdout = subprocess.PIPE\n",
    "    with subprocess.Popen(command, stdout=stdout, bufsize=1, \n",
    "                          universal_newlines=True) as process:\n",
    "        for line in process.stdout:\n",
    "            line = line[:-1]\n",
    "            outputs.append(line)\n",
    "            print(line)\n",
    "    output = ''.join(outputs)\n",
    "    return output\n",
    "\n",
    "\n",
    "def export_bf16_tensor_to_file(tensor, filename):\n",
    "    tensor = tensor.to(device='cpu')\n",
    "    tensor = tensor.float()\n",
    "    tensor = tensor.numpy()\n",
    "    tensor = tensor.astype(bfloat16)\n",
    "    tensor.tofile(filename)\n",
    "\n",
    "\n",
    "def export_bf16_inputs(sample):\n",
    "    export_bf16_tensor_to_file(sample[0], \"input.dat\")\n",
    "    export_bf16_tensor_to_file(sample[1], \"k_cache.dat\")\n",
    "    export_bf16_tensor_to_file(sample[2], \"v_cache.dat\")\n",
    "\n",
    "\n",
    "def shape_to_str(shape):\n",
    "    return \"x\".join([str(i) for i in shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ff786-f8a6-4266-9400-f71b8f06d14b",
   "metadata": {},
   "source": [
    "<br>\n",
    "First, let's set the parameters. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc008e6f-e44f-4428-8d8c-5bcb3de88c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz, seqlen, dim, core_dim, n_heads, cache_len = 4, 2048, 512, 512, 128, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33deec-e1b5-4aa1-acae-bb3b92f3fdd3",
   "metadata": {},
   "source": [
    "<br>\n",
    "Below we deploy the softmax attention layer into TensorRT format and benchmark it. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3928b1-bc27-4f3a-9443-fb0e31f03ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/onnx/utils.py:784: UserWarning: no signature found for builtin <built-in method __call__ of PyCapsule object at 0x7ff280033ab0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100900] [b34] # trtexec --onnx=./model.onnx --loadInputs=input:./input.dat --inputIOFormats=bf16:chw,bf16:chw,bf16:chw --outputIOFormats=bf16:chw,bf16:chw,bf16:chw --bf16 --minShapes=input:4x2048x512,k_cache:4x0x512,v_cache:4x0x512 --optShapes=input:4x2048x512,k_cache:4x0x512,v_cache:4x0x512 --maxShapes=input:4x2048x512,k_cache:4x0x512,v_cache:4x0x512 --builderOptimizationLevel=5 --maxAuxStreams=2 --saveEngine=./model.engine\n",
      "[10/07/2025-13:47:44] [I] === Model Options ===\n",
      "[10/07/2025-13:47:44] [I] Format: ONNX\n",
      "[10/07/2025-13:47:44] [I] Model: ./model.onnx\n",
      "[10/07/2025-13:47:44] [I] Output:\n",
      "[10/07/2025-13:47:44] [I] === Build Options ===\n",
      "[10/07/2025-13:47:44] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default, tacticSharedMem: default\n",
      "[10/07/2025-13:47:44] [I] avgTiming: 8\n",
      "[10/07/2025-13:47:44] [I] Precision: FP32+BF16\n",
      "[10/07/2025-13:47:44] [I] LayerPrecisions: \n",
      "[10/07/2025-13:47:44] [I] Layer Device Types: \n",
      "[10/07/2025-13:47:44] [I] Calibration: \n",
      "[10/07/2025-13:47:44] [I] Refit: Disabled\n",
      "[10/07/2025-13:47:44] [I] Strip weights: Disabled\n",
      "[10/07/2025-13:47:44] [I] Version Compatible: Disabled\n",
      "[10/07/2025-13:47:44] [I] ONNX Plugin InstanceNorm: Disabled\n",
      "[10/07/2025-13:47:44] [I] TensorRT runtime: full\n",
      "[10/07/2025-13:47:44] [I] Lean DLL Path: \n",
      "[10/07/2025-13:47:44] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[10/07/2025-13:47:44] [I] Exclude Lean Runtime: Disabled\n",
      "[10/07/2025-13:47:44] [I] Sparsity: Disabled\n",
      "[10/07/2025-13:47:44] [I] Safe mode: Disabled\n",
      "[10/07/2025-13:47:44] [I] Build DLA standalone loadable: Disabled\n",
      "[10/07/2025-13:47:44] [I] Allow GPU fallback for DLA: Disabled\n",
      "[10/07/2025-13:47:44] [I] DirectIO mode: Disabled\n",
      "[10/07/2025-13:47:44] [I] Restricted mode: Disabled\n",
      "[10/07/2025-13:47:44] [I] Skip inference: Disabled\n",
      "[10/07/2025-13:47:44] [I] Save engine: ./model.engine\n",
      "[10/07/2025-13:47:44] [I] Load engine: \n",
      "[10/07/2025-13:47:44] [I] Profiling verbosity: 0\n",
      "[10/07/2025-13:47:44] [I] Tactic sources: Using default tactic sources\n",
      "[10/07/2025-13:47:44] [I] timingCacheMode: local\n",
      "[10/07/2025-13:47:44] [I] timingCacheFile: \n",
      "[10/07/2025-13:47:44] [I] Enable Compilation Cache: Enabled\n",
      "[10/07/2025-13:47:44] [I] Enable Monitor Memory: Disabled\n",
      "[10/07/2025-13:47:44] [I] errorOnTimingCacheMiss: Disabled\n",
      "[10/07/2025-13:47:44] [I] Preview Features: Use default preview flags.\n",
      "[10/07/2025-13:47:44] [I] MaxAuxStreams: 2\n",
      "[10/07/2025-13:47:44] [I] BuilderOptimizationLevel: 5\n",
      "[10/07/2025-13:47:44] [I] MaxTactics: -1\n",
      "[10/07/2025-13:47:44] [I] Calibration Profile Index: 0\n",
      "[10/07/2025-13:47:44] [I] Weight Streaming: Disabled\n",
      "[10/07/2025-13:47:44] [I] Runtime Platform: Same As Build\n",
      "[10/07/2025-13:47:44] [I] Debug Tensors: \n",
      "[10/07/2025-13:47:44] [I] Input(s): bf16:chw\n",
      "[10/07/2025-13:47:44] [I] Input(s): bf16:chw\n",
      "[10/07/2025-13:47:44] [I] Input(s): bf16:chw\n",
      "[10/07/2025-13:47:44] [I] Output(s): bf16:chw\n",
      "[10/07/2025-13:47:44] [I] Output(s): bf16:chw\n",
      "[10/07/2025-13:47:44] [I] Output(s): bf16:chw\n",
      "[10/07/2025-13:47:44] [I] Input build shape (profile 0): input=4x2048x512+4x2048x512+4x2048x512\n",
      "[10/07/2025-13:47:44] [I] Input build shape (profile 0): k_cache=4x0x512+4x0x512+4x0x512\n",
      "[10/07/2025-13:47:44] [I] Input build shape (profile 0): v_cache=4x0x512+4x0x512+4x0x512\n",
      "[10/07/2025-13:47:44] [I] Input calibration shapes: model\n",
      "[10/07/2025-13:47:44] [I] === System Options ===\n",
      "[10/07/2025-13:47:44] [I] Device: 0\n",
      "[10/07/2025-13:47:44] [I] DLACore: \n",
      "[10/07/2025-13:47:44] [I] Plugins:\n",
      "[10/07/2025-13:47:44] [I] setPluginsToSerialize:\n",
      "[10/07/2025-13:47:44] [I] dynamicPlugins:\n",
      "[10/07/2025-13:47:44] [I] ignoreParsedPluginLibs: 0\n",
      "[10/07/2025-13:47:44] [I] \n",
      "[10/07/2025-13:47:44] [I] === Inference Options ===\n",
      "[10/07/2025-13:47:44] [I] Batch: Explicit\n",
      "[10/07/2025-13:47:44] [I] Input inference shape : v_cache=4x0x512\n",
      "[10/07/2025-13:47:44] [I] Input inference shape : k_cache=4x0x512\n",
      "[10/07/2025-13:47:44] [I] Input inference shape : input=4x2048x512\n",
      "[10/07/2025-13:47:44] [I] Iterations: 10\n",
      "[10/07/2025-13:47:44] [I] Duration: 3s (+ 200ms warm up)\n",
      "[10/07/2025-13:47:44] [I] Sleep time: 0ms\n",
      "[10/07/2025-13:47:44] [I] Idle time: 0ms\n",
      "[10/07/2025-13:47:44] [I] Inference Streams: 1\n",
      "[10/07/2025-13:47:44] [I] ExposeDMA: Disabled\n",
      "[10/07/2025-13:47:44] [I] Data transfers: Enabled\n",
      "[10/07/2025-13:47:44] [I] Spin-wait: Disabled\n",
      "[10/07/2025-13:47:44] [I] Multithreading: Disabled\n",
      "[10/07/2025-13:47:44] [I] CUDA Graph: Disabled\n",
      "[10/07/2025-13:47:44] [I] Separate profiling: Disabled\n",
      "[10/07/2025-13:47:44] [I] Time Deserialize: Disabled\n",
      "[10/07/2025-13:47:44] [I] Time Refit: Disabled\n",
      "[10/07/2025-13:47:44] [I] NVTX verbosity: 0\n",
      "[10/07/2025-13:47:44] [I] Persistent Cache Ratio: 0\n",
      "[10/07/2025-13:47:44] [I] Optimization Profile Index: 0\n",
      "[10/07/2025-13:47:44] [I] Weight Streaming Budget: 100.000000%\n",
      "[10/07/2025-13:47:44] [I] Inputs:\n",
      "[10/07/2025-13:47:44] [I] input<-./input.dat\n",
      "[10/07/2025-13:47:44] [I] Debug Tensor Save Destinations:\n",
      "[10/07/2025-13:47:44] [I] === Reporting Options ===\n",
      "[10/07/2025-13:47:44] [I] Verbose: Disabled\n",
      "[10/07/2025-13:47:44] [I] Averages: 10 inferences\n",
      "[10/07/2025-13:47:44] [I] Percentiles: 90,95,99\n",
      "[10/07/2025-13:47:44] [I] Dump refittable layers:Disabled\n",
      "[10/07/2025-13:47:44] [I] Dump output: Disabled\n",
      "[10/07/2025-13:47:44] [I] Profile: Disabled\n",
      "[10/07/2025-13:47:44] [I] Export timing to JSON file: \n",
      "[10/07/2025-13:47:44] [I] Export output to JSON file: \n",
      "[10/07/2025-13:47:44] [I] Export profile to JSON file: \n",
      "[10/07/2025-13:47:44] [I] \n",
      "[10/07/2025-13:47:44] [I] === Device Information ===\n",
      "[10/07/2025-13:47:44] [I] Available Devices: \n",
      "[10/07/2025-13:47:44] [I]   Device 0: \"NVIDIA GeForce RTX 4090 Laptop GPU\" UUID: GPU-53966644-cef9-faf9-710a-ac7b2b5d12d5\n",
      "[10/07/2025-13:47:44] [I] Selected Device: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "[10/07/2025-13:47:44] [I] Selected Device ID: 0\n",
      "[10/07/2025-13:47:44] [I] Selected Device UUID: GPU-53966644-cef9-faf9-710a-ac7b2b5d12d5\n",
      "[10/07/2025-13:47:44] [I] Compute Capability: 8.9\n",
      "[10/07/2025-13:47:44] [I] SMs: 76\n",
      "[10/07/2025-13:47:44] [I] Device Global Memory: 15954 MiB\n",
      "[10/07/2025-13:47:44] [I] Shared Memory per SM: 100 KiB\n",
      "[10/07/2025-13:47:44] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[10/07/2025-13:47:44] [I] Application Compute Clock Rate: 1.455 GHz\n",
      "[10/07/2025-13:47:44] [I] Application Memory Clock Rate: 9.001 GHz\n",
      "[10/07/2025-13:47:44] [I] \n",
      "[10/07/2025-13:47:44] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[10/07/2025-13:47:44] [I] \n",
      "[10/07/2025-13:47:44] [I] TensorRT version: 10.9.0\n",
      "[10/07/2025-13:47:44] [I] Loading standard plugins\n",
      "[10/07/2025-13:47:44] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 25, GPU 711 (MiB)\n",
      "[10/07/2025-13:47:46] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +2773, GPU +446, now: CPU 3000, GPU 1157 (MiB)\n",
      "[10/07/2025-13:47:46] [I] Start parsing network model.\n",
      "[10/07/2025-13:47:46] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/07/2025-13:47:46] [I] [TRT] Input filename:   ./model.onnx\n",
      "[10/07/2025-13:47:46] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[10/07/2025-13:47:46] [I] [TRT] Opset version:    18\n",
      "[10/07/2025-13:47:46] [I] [TRT] Producer name:    pytorch\n",
      "[10/07/2025-13:47:46] [I] [TRT] Producer version: 2.7.0\n",
      "[10/07/2025-13:47:46] [I] [TRT] Domain:           \n",
      "[10/07/2025-13:47:46] [I] [TRT] Model version:    0\n",
      "[10/07/2025-13:47:46] [I] [TRT] Doc string:       \n",
      "[10/07/2025-13:47:46] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/07/2025-13:47:46] [I] Finished parsing network model. Parse time: 0.00474098\n",
      "[10/07/2025-13:47:46] [I] Set shape of input tensor input for optimization profile 0 to: MIN=4x2048x512 OPT=4x2048x512 MAX=4x2048x512\n",
      "[10/07/2025-13:47:46] [I] Set shape of input tensor k_cache for optimization profile 0 to: MIN=4x0x512 OPT=4x0x512 MAX=4x0x512\n",
      "[10/07/2025-13:47:46] [I] Set shape of input tensor v_cache for optimization profile 0 to: MIN=4x0x512 OPT=4x0x512 MAX=4x0x512\n",
      "[10/07/2025-13:47:46] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/07/2025-13:47:46] [I] [TRT] Compiler backend is used during engine build.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:48:01] [W] [TRT] Tactic Device request: 16512MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:01] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17314086912 detected for tactic 0x0000000000000000.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000000.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000001.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 2 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000002.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 3 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000003.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 4 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000004.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 5 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000005.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 6 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000006.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 7 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000007.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 8 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000008.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 9 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000009.\n",
      "[10/07/2025-13:48:06] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:06] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 10 due to insufficient memory on requested size of 17196646400 detected for tactic 0x000000000000001c.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000000.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000001.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 2 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000002.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 3 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000003.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 4 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000004.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 5 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000005.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 6 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000006.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 7 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000007.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 8 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000008.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 9 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000009.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 10 due to insufficient memory on requested size of 17196646400 detected for tactic 0x000000000000001c.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000000.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000001.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 2 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000002.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 3 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000003.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 4 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000004.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 5 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000005.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 6 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000006.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 7 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000007.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 8 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000008.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 9 due to insufficient memory on requested size of 17196646400 detected for tactic 0x0000000000000009.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16400MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 10 due to insufficient memory on requested size of 17196646400 detected for tactic 0x000000000000001c.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000000.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000001.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 2 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000002.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 3 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000003.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 4 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000004.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 5 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000005.\n",
      "[10/07/2025-13:48:07] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:07] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 6 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000006.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 7 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000007.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 8 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000008.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 9 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000009.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 10 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000a.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 11 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000b.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 12 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000c.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 13 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000d.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 14 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000e.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 15 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000f.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 16 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000010.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 17 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000011.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 18 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000012.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 19 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000013.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 20 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000014.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 21 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000015.\n",
      "[10/07/2025-13:48:08] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:08] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 22 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000016.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 23 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000017.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 24 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000001c.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 25 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000001d.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 26 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000001e.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16896MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17716740096 detected for tactic 0x0000000000000018.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16896MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 17716740096 detected for tactic 0x0000000000000019.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16896MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 2 due to insufficient memory on requested size of 17716740096 detected for tactic 0x000000000000001a.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16896MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 3 due to insufficient memory on requested size of 17716740096 detected for tactic 0x000000000000001b.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16896MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 4 due to insufficient memory on requested size of 17716740096 detected for tactic 0x000000000000001f.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000000.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 1 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000001.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 2 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000002.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 3 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000003.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 4 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000004.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 5 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000005.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 6 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000006.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 7 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000007.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 8 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000008.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 9 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000009.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 10 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000a.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 11 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000b.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 12 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000c.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 13 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000d.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 14 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000e.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 15 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000000f.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 16 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000010.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 17 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000011.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 18 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000012.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 19 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000013.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 20 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000014.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 21 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000015.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 22 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000016.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 23 due to insufficient memory on requested size of 17246978048 detected for tactic 0x0000000000000017.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 24 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000001c.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 25 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000001d.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Tactic Device request: 16448MB Available: 15954MB. Device memory is insufficient to use tactic.\n",
      "[10/07/2025-13:48:09] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 26 due to insufficient memory on requested size of 17246978048 detected for tactic 0x000000000000001e.\n",
      "[10/07/2025-13:48:09] [W] [TRT] Engine generation failed with backend strategy 4.\n",
      "Error message: [optimizer.cpp::computeCosts::4157] Error Code 10: Internal Error (Could not find any implementation for node PWN(/Add).).\n",
      "Skipping this backend strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:48:09] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/07/2025-13:48:09] [I] [TRT] Compiler backend is used during engine build.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:48:27] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17280532480 detected for tactic 0x0000000000000000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:48:39] [I] [TRT] [MS] Multi stream is disabled as cannot find an opportunity to leverage it\n",
      "[10/07/2025-13:48:39] [I] [TRT] Detected 3 inputs and 3 output network tensors.\n",
      "[10/07/2025-13:48:39] [I] [TRT] Total Host Persistent Memory: 80 bytes\n",
      "[10/07/2025-13:48:39] [I] [TRT] Total Device Persistent Memory: 0 bytes\n",
      "[10/07/2025-13:48:39] [I] [TRT] Max Scratch Memory: 8682209280 bytes\n",
      "[10/07/2025-13:48:39] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.\n",
      "[10/07/2025-13:48:39] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.009499ms to assign 1 blocks to 1 nodes requiring 8682209280 bytes.\n",
      "[10/07/2025-13:48:39] [I] [TRT] Total Activation Memory: 8682209280 bytes\n",
      "[10/07/2025-13:48:39] [I] [TRT] Total Weights Memory: 10486144 bytes\n",
      "[10/07/2025-13:48:39] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[10/07/2025-13:48:39] [I] [TRT] Engine generation completed in 30.0939 seconds.\n",
      "[10/07/2025-13:48:39] [I] [TRT] [MS] Running engine with multi stream info\n",
      "[10/07/2025-13:48:39] [I] [TRT] [MS] Number of aux streams is 2\n",
      "[10/07/2025-13:48:39] [I] [TRT] [MS] Number of total worker streams is 3\n",
      "[10/07/2025-13:48:39] [I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[10/07/2025-13:48:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +8280, now: CPU 0, GPU 8290 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:48:39] [W] [TRT] [MS] Multi stream is disabled because the stream assignment failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:48:40] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/07/2025-13:48:40] [I] [TRT] Compiler backend is used during engine build.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:48:58] [W] [TRT] UNSUPPORTED_STATE: Skipping tactic 0 due to insufficient memory on requested size of 17280532480 detected for tactic 0x0000000000000000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:49:10] [I] [TRT] [MS] Multi stream is disabled as cannot find an opportunity to leverage it\n",
      "[10/07/2025-13:49:10] [I] [TRT] Detected 3 inputs and 3 output network tensors.\n",
      "[10/07/2025-13:49:10] [I] [TRT] Total Host Persistent Memory: 80 bytes\n",
      "[10/07/2025-13:49:10] [I] [TRT] Total Device Persistent Memory: 0 bytes\n",
      "[10/07/2025-13:49:10] [I] [TRT] Max Scratch Memory: 8682209280 bytes\n",
      "[10/07/2025-13:49:10] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.\n",
      "[10/07/2025-13:49:10] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.008312ms to assign 1 blocks to 1 nodes requiring 8682209280 bytes.\n",
      "[10/07/2025-13:49:10] [I] [TRT] Total Activation Memory: 8682209280 bytes\n",
      "[10/07/2025-13:49:10] [I] [TRT] Total Weights Memory: 10486144 bytes\n",
      "[10/07/2025-13:49:10] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[10/07/2025-13:49:10] [I] [TRT] Engine generation completed in 29.9979 seconds.\n",
      "[10/07/2025-13:49:10] [I] [TRT] [MS] Running engine with multi stream info\n",
      "[10/07/2025-13:49:10] [I] [TRT] [MS] Number of aux streams is 2\n",
      "[10/07/2025-13:49:10] [I] [TRT] [MS] Number of total worker streams is 3\n",
      "[10/07/2025-13:49:10] [I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[10/07/2025-13:49:10] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +8280, now: CPU 0, GPU 8300 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:49:10] [W] [TRT] [MS] Multi stream is disabled because the stream assignment failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:49:11] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 3 MiB, GPU 8332 MiB\n",
      "[10/07/2025-13:49:11] [I] Engine built in 84.6637 sec.\n",
      "[10/07/2025-13:49:11] [I] Created engine with size: 10.7329 MiB\n",
      "[10/07/2025-13:49:11] [I] [TRT] Loaded engine size: 10 MiB\n",
      "[10/07/2025-13:49:11] [I] Engine deserialized in 0.00816047 sec.\n",
      "[10/07/2025-13:49:11] [I] [TRT] [MS] Running engine with multi stream info\n",
      "[10/07/2025-13:49:11] [I] [TRT] [MS] Number of aux streams is 2\n",
      "[10/07/2025-13:49:11] [I] [TRT] [MS] Number of total worker streams is 3\n",
      "[10/07/2025-13:49:11] [I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[10/07/2025-13:49:11] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +8280, now: CPU 0, GPU 8290 (MiB)\n",
      "[10/07/2025-13:49:11] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[10/07/2025-13:49:11] [I] Created execution context with device memory size: 8280 MiB\n",
      "[10/07/2025-13:49:11] [I] Using values loaded from ./input.dat for input input\n",
      "[10/07/2025-13:49:11] [I] Input binding for input with dimensions 4x2048x512 is created.\n",
      "[10/07/2025-13:49:11] [I] Using random values for input k_cache\n",
      "[10/07/2025-13:49:11] [I] Input binding for k_cache with dimensions 4x0x512 is created.\n",
      "[10/07/2025-13:49:11] [I] Using random values for input v_cache\n",
      "[10/07/2025-13:49:11] [I] Input binding for v_cache with dimensions 4x0x512 is created.\n",
      "[10/07/2025-13:49:11] [I] Output binding for output with dimensions 4x2048x512 is created.\n",
      "[10/07/2025-13:49:11] [I] Output binding for updated_k_cache with dimensions 4x2048x512 is created.\n",
      "[10/07/2025-13:49:11] [I] Output binding for updated_v_cache with dimensions 4x2048x512 is created.\n",
      "[10/07/2025-13:49:11] [I] Starting inference\n",
      "[10/07/2025-13:49:14] [I] Warmup completed 5 queries over 200 ms\n",
      "[10/07/2025-13:49:14] [I] Timing trace has 61 queries over 3.1527 s\n",
      "[10/07/2025-13:49:14] [I] \n",
      "[10/07/2025-13:49:14] [I] === Trace details ===\n",
      "[10/07/2025-13:49:14] [I] Trace averages of 10 runs:\n",
      "[10/07/2025-13:49:14] [I] Average on 10 runs - GPU latency: 53.9186 ms - Host latency: 55.4081 ms (enqueue 0.0612686 ms)\n",
      "[10/07/2025-13:49:14] [I] Average on 10 runs - GPU latency: 51.5369 ms - Host latency: 54.1108 ms (enqueue 0.0654358 ms)\n",
      "[10/07/2025-13:49:14] [I] Average on 10 runs - GPU latency: 49.7796 ms - Host latency: 52.4284 ms (enqueue 0.0665894 ms)\n",
      "[10/07/2025-13:49:14] [I] Average on 10 runs - GPU latency: 49.9 ms - Host latency: 52.5455 ms (enqueue 0.0599731 ms)\n",
      "[10/07/2025-13:49:14] [I] Average on 10 runs - GPU latency: 49.9449 ms - Host latency: 52.5853 ms (enqueue 0.0599854 ms)\n",
      "[10/07/2025-13:49:14] [I] Average on 10 runs - GPU latency: 49.8054 ms - Host latency: 52.4451 ms (enqueue 0.0533936 ms)\n",
      "[10/07/2025-13:49:14] [I] \n",
      "[10/07/2025-13:49:14] [I] === Performance summary ===\n",
      "[10/07/2025-13:49:14] [I] Throughput: 19.3485 qps\n",
      "[10/07/2025-13:49:14] [I] Latency: min = 51.603 ms, max = 61.756 ms, mean = 53.2391 ms, median = 52.5144 ms, percentile(90%) = 55.6101 ms, percentile(95%) = 55.785 ms, percentile(99%) = 61.756 ms\n",
      "[10/07/2025-13:49:14] [I] Enqueue Time: min = 0.0392456 ms, max = 0.130585 ms, mean = 0.0611585 ms, median = 0.0627441 ms, percentile(90%) = 0.0700684 ms, percentile(95%) = 0.0758057 ms, percentile(99%) = 0.130585 ms\n",
      "[10/07/2025-13:49:14] [I] H2D Latency: min = 0.36554 ms, max = 0.704712 ms, mean = 0.624823 ms, median = 0.681641 ms, percentile(90%) = 0.695801 ms, percentile(95%) = 0.699463 ms, percentile(99%) = 0.704712 ms\n",
      "[10/07/2025-13:49:14] [I] GPU Compute Time: min = 48.9463 ms, max = 59.436 ms, mean = 50.7974 ms, median = 49.8782 ms, percentile(90%) = 54.1389 ms, percentile(95%) = 54.3048 ms, percentile(99%) = 59.436 ms\n",
      "[10/07/2025-13:49:14] [I] D2H Latency: min = 1.08173 ms, max = 1.97644 ms, mean = 1.81687 ms, median = 1.95459 ms, percentile(90%) = 1.96777 ms, percentile(95%) = 1.97168 ms, percentile(99%) = 1.97644 ms\n",
      "[10/07/2025-13:49:14] [I] Total Host Walltime: 3.1527 s\n",
      "[10/07/2025-13:49:14] [I] Total GPU Compute Time: 3.09864 s\n",
      "[10/07/2025-13:49:14] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[10/07/2025-13:49:14] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100900] [b34] # trtexec --onnx=./model.onnx --loadInputs=input:./input.dat --inputIOFormats=bf16:chw,bf16:chw,bf16:chw --outputIOFormats=bf16:chw,bf16:chw,bf16:chw --bf16 --minShapes=input:4x2048x512,k_cache:4x0x512,v_cache:4x0x512 --optShapes=input:4x2048x512,k_cache:4x0x512,v_cache:4x0x512 --maxShapes=input:4x2048x512,k_cache:4x0x512,v_cache:4x0x512 --builderOptimizationLevel=5 --maxAuxStreams=2 --saveEngine=./model.engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:49:14] [W] * GPU compute time is unstable, with coefficient of variance = 3.8752%.\n",
      "[10/07/2025-13:49:14] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n"
     ]
    }
   ],
   "source": [
    "# create the model \n",
    "softmax_attention = AttentionLayerSM(dim, n_heads).cuda().bfloat16()\n",
    "# create a sample input \n",
    "x = get_input(bsz, seqlen, dim, 'cuda', torch.bfloat16)\n",
    "k_cache, v_cache = get_sm_cache(bsz, cache_len, dim, 'cuda', torch.bfloat16)\n",
    "sample = (x, k_cache, v_cache)\n",
    "\n",
    "# warmup and recompilation of torchscript models may take too long to run.... \n",
    "# especially for large models\n",
    "# here we just turn off graph optimizations to deal with that\n",
    "with torch.no_grad() and torch.jit.optimized_execution(False):\n",
    "    # ts export\n",
    "    softmax_attention_traced = torch.jit.trace(softmax_attention, sample, check_trace=False)\n",
    "    \n",
    "    # onnx export\n",
    "    onnx_model_path = \"model.onnx\"\n",
    "    torch.onnx.export(softmax_attention_traced, sample, onnx_model_path, verbose=False, \n",
    "                      opset_version=18, export_params=True, \n",
    "                      keep_initializers_as_inputs=False, \n",
    "                      custom_opsets={}, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['input', 'k_cache', 'v_cache'], \n",
    "                      output_names=['output', 'updated_k_cache', 'updated_v_cache'], \n",
    "                      dynamic_axes={'input' : {0 : 'batch_size', 1 : 'query_len'}, \n",
    "                                    'k_cache' : {0 : 'batch_size', 2 : 'cache_len'}, \n",
    "                                    'v_cache' : {0 : 'batch_size', 2 : 'cache_len'}, \n",
    "                                    'output' : {0 : 'batch_size', 1 : 'query_len'}, \n",
    "                                    'updated_k_cache' : {0 : 'batch_size', 2 : 'updated_cache_len'}, \n",
    "                                    'updated_v_cache' : {0 : 'batch_size', 2 : 'updated_cache_len'}})\n",
    "\n",
    "# tensorrt export\n",
    "export_bf16_inputs(sample)\n",
    "\n",
    "command = \"trtexec --onnx=./model.onnx\"\n",
    "command += \" --loadInputs=input:./input.dat\"\n",
    "if k_cache.numel() > 0:\n",
    "    command += \",k_cache:./k_cache.dat,v_cache:./v_cache.dat\"\n",
    "command += \" --inputIOFormats=bf16:chw,bf16:chw,bf16:chw\"\n",
    "command += \" --outputIOFormats=bf16:chw,bf16:chw,bf16:chw\"\n",
    "command += \" --bf16\"\n",
    "input_shape = shape_to_str(sample[0].shape)\n",
    "k_cache_shape = shape_to_str(sample[1].shape)\n",
    "v_cache_shape = shape_to_str(sample[2].shape)\n",
    "shapes = \"input:\" + input_shape + \",k_cache:\" + k_cache_shape + \",v_cache:\" + v_cache_shape\n",
    "command += \" --minShapes=\" + shapes\n",
    "command += \" --optShapes=\" + shapes\n",
    "command += \" --maxShapes=\" + shapes\n",
    "command += \" --builderOptimizationLevel=5\"\n",
    "command += \" --maxAuxStreams=2\"  # the larger `maxAuxStreams` is, the more memory the engine needs! \n",
    "#command += \" --memPoolSize=workspace:16384\"\n",
    "command += \" --saveEngine=./model.engine\"\n",
    "# command += \" --skipInference\"\n",
    "execute(command)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4251a-dc44-45d6-a1c0-737c61bcece6",
   "metadata": {},
   "source": [
    "<br>\n",
    "Below we deploy the softplus attention layer into TensorRT format and benchmark it. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792556d9-8f15-4ec2-8008-9f7ddb519e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/onnx/utils.py:784: UserWarning: no signature found for builtin <built-in method __call__ of PyCapsule object at 0x7ff280033ab0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v100900] [b34] # trtexec --onnx=./model.onnx --staticPlugins=/usr/local/lib/python3.12/dist-packages/bluewalm/operators/tensorrt/libbluewalmPlugin.so --loadInputs=input:./input.dat --inputIOFormats=bf16:chw,bf16:chw,bf16:chw --outputIOFormats=bf16:chw,bf16:chw,bf16:chw --bf16 --minShapes=input:4x2048x512,k_cache:4x512x0,v_cache:4x512x0 --optShapes=input:4x2048x512,k_cache:4x512x0,v_cache:4x512x0 --maxShapes=input:4x2048x512,k_cache:4x512x0,v_cache:4x512x0 --builderOptimizationLevel=5 --maxAuxStreams=2 --saveEngine=./model.engine\n",
      "[10/07/2025-13:49:15] [I] === Model Options ===\n",
      "[10/07/2025-13:49:15] [I] Format: ONNX\n",
      "[10/07/2025-13:49:15] [I] Model: ./model.onnx\n",
      "[10/07/2025-13:49:15] [I] Output:\n",
      "[10/07/2025-13:49:15] [I] === Build Options ===\n",
      "[10/07/2025-13:49:15] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default, tacticSharedMem: default\n",
      "[10/07/2025-13:49:15] [I] avgTiming: 8\n",
      "[10/07/2025-13:49:15] [I] Precision: FP32+BF16\n",
      "[10/07/2025-13:49:15] [I] LayerPrecisions: \n",
      "[10/07/2025-13:49:15] [I] Layer Device Types: \n",
      "[10/07/2025-13:49:15] [I] Calibration: \n",
      "[10/07/2025-13:49:15] [I] Refit: Disabled\n",
      "[10/07/2025-13:49:15] [I] Strip weights: Disabled\n",
      "[10/07/2025-13:49:15] [I] Version Compatible: Disabled\n",
      "[10/07/2025-13:49:15] [I] ONNX Plugin InstanceNorm: Disabled\n",
      "[10/07/2025-13:49:15] [I] TensorRT runtime: full\n",
      "[10/07/2025-13:49:15] [I] Lean DLL Path: \n",
      "[10/07/2025-13:49:15] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[10/07/2025-13:49:15] [I] Exclude Lean Runtime: Disabled\n",
      "[10/07/2025-13:49:15] [I] Sparsity: Disabled\n",
      "[10/07/2025-13:49:15] [I] Safe mode: Disabled\n",
      "[10/07/2025-13:49:15] [I] Build DLA standalone loadable: Disabled\n",
      "[10/07/2025-13:49:15] [I] Allow GPU fallback for DLA: Disabled\n",
      "[10/07/2025-13:49:15] [I] DirectIO mode: Disabled\n",
      "[10/07/2025-13:49:15] [I] Restricted mode: Disabled\n",
      "[10/07/2025-13:49:15] [I] Skip inference: Disabled\n",
      "[10/07/2025-13:49:15] [I] Save engine: ./model.engine\n",
      "[10/07/2025-13:49:15] [I] Load engine: \n",
      "[10/07/2025-13:49:15] [I] Profiling verbosity: 0\n",
      "[10/07/2025-13:49:15] [I] Tactic sources: Using default tactic sources\n",
      "[10/07/2025-13:49:15] [I] timingCacheMode: local\n",
      "[10/07/2025-13:49:15] [I] timingCacheFile: \n",
      "[10/07/2025-13:49:15] [I] Enable Compilation Cache: Enabled\n",
      "[10/07/2025-13:49:15] [I] Enable Monitor Memory: Disabled\n",
      "[10/07/2025-13:49:15] [I] errorOnTimingCacheMiss: Disabled\n",
      "[10/07/2025-13:49:15] [I] Preview Features: Use default preview flags.\n",
      "[10/07/2025-13:49:15] [I] MaxAuxStreams: 2\n",
      "[10/07/2025-13:49:15] [I] BuilderOptimizationLevel: 5\n",
      "[10/07/2025-13:49:15] [I] MaxTactics: -1\n",
      "[10/07/2025-13:49:15] [I] Calibration Profile Index: 0\n",
      "[10/07/2025-13:49:15] [I] Weight Streaming: Disabled\n",
      "[10/07/2025-13:49:15] [I] Runtime Platform: Same As Build\n",
      "[10/07/2025-13:49:15] [I] Debug Tensors: \n",
      "[10/07/2025-13:49:15] [I] Input(s): bf16:chw\n",
      "[10/07/2025-13:49:15] [I] Input(s): bf16:chw\n",
      "[10/07/2025-13:49:15] [I] Input(s): bf16:chw\n",
      "[10/07/2025-13:49:15] [I] Output(s): bf16:chw\n",
      "[10/07/2025-13:49:15] [I] Output(s): bf16:chw\n",
      "[10/07/2025-13:49:15] [I] Output(s): bf16:chw\n",
      "[10/07/2025-13:49:15] [I] Input build shape (profile 0): input=4x2048x512+4x2048x512+4x2048x512\n",
      "[10/07/2025-13:49:15] [I] Input build shape (profile 0): k_cache=4x512x0+4x512x0+4x512x0\n",
      "[10/07/2025-13:49:15] [I] Input build shape (profile 0): v_cache=4x512x0+4x512x0+4x512x0\n",
      "[10/07/2025-13:49:15] [I] Input calibration shapes: model\n",
      "[10/07/2025-13:49:15] [I] === System Options ===\n",
      "[10/07/2025-13:49:15] [I] Device: 0\n",
      "[10/07/2025-13:49:15] [I] DLACore: \n",
      "[10/07/2025-13:49:15] [I] Plugins: /usr/local/lib/python3.12/dist-packages/bluewalm/operators/tensorrt/libbluewalmPlugin.so\n",
      "[10/07/2025-13:49:15] [I] setPluginsToSerialize:\n",
      "[10/07/2025-13:49:15] [I] dynamicPlugins:\n",
      "[10/07/2025-13:49:15] [I] ignoreParsedPluginLibs: 0\n",
      "[10/07/2025-13:49:15] [I] \n",
      "[10/07/2025-13:49:15] [I] === Inference Options ===\n",
      "[10/07/2025-13:49:15] [I] Batch: Explicit\n",
      "[10/07/2025-13:49:15] [I] Input inference shape : v_cache=4x512x0\n",
      "[10/07/2025-13:49:15] [I] Input inference shape : k_cache=4x512x0\n",
      "[10/07/2025-13:49:15] [I] Input inference shape : input=4x2048x512\n",
      "[10/07/2025-13:49:15] [I] Iterations: 10\n",
      "[10/07/2025-13:49:15] [I] Duration: 3s (+ 200ms warm up)\n",
      "[10/07/2025-13:49:15] [I] Sleep time: 0ms\n",
      "[10/07/2025-13:49:15] [I] Idle time: 0ms\n",
      "[10/07/2025-13:49:15] [I] Inference Streams: 1\n",
      "[10/07/2025-13:49:15] [I] ExposeDMA: Disabled\n",
      "[10/07/2025-13:49:15] [I] Data transfers: Enabled\n",
      "[10/07/2025-13:49:15] [I] Spin-wait: Disabled\n",
      "[10/07/2025-13:49:15] [I] Multithreading: Disabled\n",
      "[10/07/2025-13:49:15] [I] CUDA Graph: Disabled\n",
      "[10/07/2025-13:49:15] [I] Separate profiling: Disabled\n",
      "[10/07/2025-13:49:15] [I] Time Deserialize: Disabled\n",
      "[10/07/2025-13:49:15] [I] Time Refit: Disabled\n",
      "[10/07/2025-13:49:15] [I] NVTX verbosity: 0\n",
      "[10/07/2025-13:49:15] [I] Persistent Cache Ratio: 0\n",
      "[10/07/2025-13:49:15] [I] Optimization Profile Index: 0\n",
      "[10/07/2025-13:49:15] [I] Weight Streaming Budget: 100.000000%\n",
      "[10/07/2025-13:49:15] [I] Inputs:\n",
      "[10/07/2025-13:49:15] [I] input<-./input.dat\n",
      "[10/07/2025-13:49:15] [I] Debug Tensor Save Destinations:\n",
      "[10/07/2025-13:49:15] [I] === Reporting Options ===\n",
      "[10/07/2025-13:49:15] [I] Verbose: Disabled\n",
      "[10/07/2025-13:49:15] [I] Averages: 10 inferences\n",
      "[10/07/2025-13:49:15] [I] Percentiles: 90,95,99\n",
      "[10/07/2025-13:49:15] [I] Dump refittable layers:Disabled\n",
      "[10/07/2025-13:49:15] [I] Dump output: Disabled\n",
      "[10/07/2025-13:49:15] [I] Profile: Disabled\n",
      "[10/07/2025-13:49:15] [I] Export timing to JSON file: \n",
      "[10/07/2025-13:49:15] [I] Export output to JSON file: \n",
      "[10/07/2025-13:49:15] [I] Export profile to JSON file: \n",
      "[10/07/2025-13:49:15] [I] \n",
      "[10/07/2025-13:49:15] [I] === Device Information ===\n",
      "[10/07/2025-13:49:15] [I] Available Devices: \n",
      "[10/07/2025-13:49:15] [I]   Device 0: \"NVIDIA GeForce RTX 4090 Laptop GPU\" UUID: GPU-53966644-cef9-faf9-710a-ac7b2b5d12d5\n",
      "[10/07/2025-13:49:15] [I] Selected Device: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "[10/07/2025-13:49:15] [I] Selected Device ID: 0\n",
      "[10/07/2025-13:49:15] [I] Selected Device UUID: GPU-53966644-cef9-faf9-710a-ac7b2b5d12d5\n",
      "[10/07/2025-13:49:15] [I] Compute Capability: 8.9\n",
      "[10/07/2025-13:49:15] [I] SMs: 76\n",
      "[10/07/2025-13:49:15] [I] Device Global Memory: 15954 MiB\n",
      "[10/07/2025-13:49:15] [I] Shared Memory per SM: 100 KiB\n",
      "[10/07/2025-13:49:15] [I] Memory Bus Width: 256 bits (ECC disabled)\n",
      "[10/07/2025-13:49:15] [I] Application Compute Clock Rate: 1.455 GHz\n",
      "[10/07/2025-13:49:15] [I] Application Memory Clock Rate: 9.001 GHz\n",
      "[10/07/2025-13:49:15] [I] \n",
      "[10/07/2025-13:49:15] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[10/07/2025-13:49:15] [I] \n",
      "[10/07/2025-13:49:15] [I] TensorRT version: 10.9.0\n",
      "[10/07/2025-13:49:15] [I] Loading standard plugins\n",
      "[10/07/2025-13:49:15] [I] Loading supplied plugin library: /usr/local/lib/python3.12/dist-packages/bluewalm/operators/tensorrt/libbluewalmPlugin.so\n",
      "[10/07/2025-13:49:15] [I] [TRT] [MemUsageChange] Init CUDA: CPU +2, GPU +0, now: CPU 30, GPU 713 (MiB)\n",
      "[10/07/2025-13:49:17] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +2774, GPU +446, now: CPU 3005, GPU 1159 (MiB)\n",
      "[10/07/2025-13:49:17] [I] Start parsing network model.\n",
      "[10/07/2025-13:49:17] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/07/2025-13:49:17] [I] [TRT] Input filename:   ./model.onnx\n",
      "[10/07/2025-13:49:17] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[10/07/2025-13:49:17] [I] [TRT] Opset version:    18\n",
      "[10/07/2025-13:49:17] [I] [TRT] Producer name:    pytorch\n",
      "[10/07/2025-13:49:17] [I] [TRT] Producer version: 2.7.0\n",
      "[10/07/2025-13:49:17] [I] [TRT] Domain:           \n",
      "[10/07/2025-13:49:17] [I] [TRT] Model version:    0\n",
      "[10/07/2025-13:49:17] [I] [TRT] Doc string:       \n",
      "[10/07/2025-13:49:17] [I] [TRT] ----------------------------------------------------------------\n",
      "[10/07/2025-13:49:17] [I] [TRT] No checker registered for op: linear_operator_r. Attempting to check as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] No importer registered for op: linear_operator_r. Attempting to import as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] Searching for plugin: linear_operator_r, plugin_version: 1, plugin_namespace: \n",
      "[10/07/2025-13:49:17] [I] [TRT] Successfully created plugin: linear_operator_r\n",
      "[10/07/2025-13:49:17] [I] [TRT] No checker registered for op: linear_operator_l. Attempting to check as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] No importer registered for op: linear_operator_l. Attempting to import as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] Searching for plugin: linear_operator_l, plugin_version: 1, plugin_namespace: \n",
      "[10/07/2025-13:49:17] [I] [TRT] Successfully created plugin: linear_operator_l\n",
      "[10/07/2025-13:49:17] [I] [TRT] No checker registered for op: linear_operator_l. Attempting to check as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] No importer registered for op: linear_operator_l. Attempting to import as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] Searching for plugin: linear_operator_l, plugin_version: 1, plugin_namespace: \n",
      "[10/07/2025-13:49:17] [I] [TRT] Successfully created plugin: linear_operator_l\n",
      "[10/07/2025-13:49:17] [I] [TRT] No checker registered for op: attention_operator. Attempting to check as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] No importer registered for op: attention_operator. Attempting to import as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] Searching for plugin: attention_operator, plugin_version: 1, plugin_namespace: \n",
      "[10/07/2025-13:49:17] [I] [TRT] Successfully created plugin: attention_operator\n",
      "[10/07/2025-13:49:17] [I] [TRT] No checker registered for op: linear_operator_r. Attempting to check as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] No importer registered for op: linear_operator_r. Attempting to import as plugin.\n",
      "[10/07/2025-13:49:17] [I] [TRT] Searching for plugin: linear_operator_r, plugin_version: 1, plugin_namespace: \n",
      "[10/07/2025-13:49:17] [I] [TRT] Successfully created plugin: linear_operator_r\n",
      "[10/07/2025-13:49:17] [I] Finished parsing network model. Parse time: 0.0108058\n",
      "[10/07/2025-13:49:17] [I] Set shape of input tensor input for optimization profile 0 to: MIN=4x2048x512 OPT=4x2048x512 MAX=4x2048x512\n",
      "[10/07/2025-13:49:17] [I] Set shape of input tensor k_cache for optimization profile 0 to: MIN=4x512x0 OPT=4x512x0 MAX=4x512x0\n",
      "[10/07/2025-13:49:17] [I] Set shape of input tensor v_cache for optimization profile 0 to: MIN=4x512x0 OPT=4x512x0 MAX=4x512x0\n",
      "[10/07/2025-13:49:17] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Compiler backend is used during engine build.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of streams used is 3\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of events used is 5\n",
      "[10/07/2025-13:49:18] [I] [TRT] Detected 3 inputs and 3 output network tensors.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Host Persistent Memory: 2800 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Device Persistent Memory: 0 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Max Scratch Memory: 21029892 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 10 steps to complete.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.027309ms to assign 7 blocks to 10 nodes requiring 50391040 bytes.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Activation Memory: 50390528 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Weights Memory: 2097152 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Engine generation completed in 0.413829 seconds.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Running engine with multi stream info\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of aux streams is 2\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of total worker streams is 3\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +48, now: CPU 0, GPU 50 (MiB)\n",
      "[10/07/2025-13:49:18] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Compiler backend is used during engine build.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of streams used is 3\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of events used is 6\n",
      "[10/07/2025-13:49:18] [I] [TRT] Detected 3 inputs and 3 output network tensors.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Host Persistent Memory: 2800 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Device Persistent Memory: 0 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Max Scratch Memory: 21029892 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 10 steps to complete.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.034432ms to assign 9 blocks to 10 nodes requiring 67168256 bytes.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Activation Memory: 67167744 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Weights Memory: 2097152 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Engine generation completed in 0.276452 seconds.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Running engine with multi stream info\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of aux streams is 2\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of total worker streams is 3\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +64, now: CPU 0, GPU 68 (MiB)\n",
      "[10/07/2025-13:49:18] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Compiler backend is used during engine build.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of streams used is 3\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of events used is 5\n",
      "[10/07/2025-13:49:18] [I] [TRT] Detected 3 inputs and 3 output network tensors.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Host Persistent Memory: 2800 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Device Persistent Memory: 0 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Max Scratch Memory: 21029892 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 10 steps to complete.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.032057ms to assign 8 blocks to 10 nodes requiring 62973952 bytes.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Activation Memory: 62973440 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Total Weights Memory: 2097152 bytes\n",
      "[10/07/2025-13:49:18] [I] [TRT] Compiler backend is used during engine execution.\n",
      "[10/07/2025-13:49:18] [I] [TRT] Engine generation completed in 0.279044 seconds.\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Running engine with multi stream info\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of aux streams is 2\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] Number of total worker streams is 3\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +60, now: CPU 0, GPU 64 (MiB)\n",
      "[10/07/2025-13:49:18] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 103 MiB\n",
      "[10/07/2025-13:49:18] [I] Engine built in 1.00641 sec.\n",
      "[10/07/2025-13:49:18] [I] Created engine with size: 2.02626 MiB\n",
      "[10/07/2025-13:49:19] [I] [TRT] Loaded engine size: 2 MiB\n",
      "[10/07/2025-13:49:19] [I] Engine deserialized in 0.00601854 sec.\n",
      "[10/07/2025-13:49:19] [I] [TRT] [MS] Running engine with multi stream info\n",
      "[10/07/2025-13:49:19] [I] [TRT] [MS] Number of aux streams is 2\n",
      "[10/07/2025-13:49:19] [I] [TRT] [MS] Number of total worker streams is 3\n",
      "[10/07/2025-13:49:19] [I] [TRT] [MS] The main stream provided by execute/enqueue calls is the first worker stream\n",
      "[10/07/2025-13:49:19] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +48, now: CPU 0, GPU 50 (MiB)\n",
      "[10/07/2025-13:49:19] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[10/07/2025-13:49:19] [I] Created execution context with device memory size: 48.0562 MiB\n",
      "[10/07/2025-13:49:19] [I] Using values loaded from ./input.dat for input input\n",
      "[10/07/2025-13:49:19] [I] Input binding for input with dimensions 4x2048x512 is created.\n",
      "[10/07/2025-13:49:19] [I] Using random values for input k_cache\n",
      "[10/07/2025-13:49:19] [I] Input binding for k_cache with dimensions 4x512x0 is created.\n",
      "[10/07/2025-13:49:19] [I] Using random values for input v_cache\n",
      "[10/07/2025-13:49:19] [I] Input binding for v_cache with dimensions 4x512x0 is created.\n",
      "[10/07/2025-13:49:19] [I] Output binding for output with dimensions 4x2048x512 is created.\n",
      "[10/07/2025-13:49:19] [I] Output binding for updated_k_cache with dimensions 4x512x2048 is created.\n",
      "[10/07/2025-13:49:19] [I] Output binding for updated_v_cache with dimensions 4x512x2048 is created.\n",
      "[10/07/2025-13:49:19] [I] Starting inference\n",
      "[10/07/2025-13:49:22] [I] Warmup completed 156 queries over 200 ms\n",
      "[10/07/2025-13:49:22] [I] Timing trace has 2404 queries over 3.00357 s\n",
      "[10/07/2025-13:49:22] [I] \n",
      "[10/07/2025-13:49:22] [I] === Trace details ===\n",
      "[10/07/2025-13:49:22] [I] Trace averages of 10 runs:\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.682701 ms - Host latency: 2.36886 ms (enqueue 0.0449982 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.677991 ms - Host latency: 2.332 ms (enqueue 0.0445236 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.671642 ms - Host latency: 2.34463 ms (enqueue 0.0425079 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.667752 ms - Host latency: 2.31506 ms (enqueue 0.0413147 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.658943 ms - Host latency: 2.34237 ms (enqueue 0.0511703 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.661194 ms - Host latency: 2.3139 ms (enqueue 0.0453583 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659042 ms - Host latency: 2.31119 ms (enqueue 0.0455902 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.652621 ms - Host latency: 2.33651 ms (enqueue 0.043811 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.657126 ms - Host latency: 2.33651 ms (enqueue 0.0482056 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.647476 ms - Host latency: 2.37226 ms (enqueue 0.0561676 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656491 ms - Host latency: 2.32608 ms (enqueue 0.060675 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649011 ms - Host latency: 2.31875 ms (enqueue 0.052356 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.650552 ms - Host latency: 2.31461 ms (enqueue 0.0586609 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649319 ms - Host latency: 2.30537 ms (enqueue 0.053064 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656274 ms - Host latency: 2.31471 ms (enqueue 0.0553345 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653925 ms - Host latency: 2.31234 ms (enqueue 0.0595947 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.657297 ms - Host latency: 2.34258 ms (enqueue 0.0466583 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655966 ms - Host latency: 2.29521 ms (enqueue 0.0573486 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654745 ms - Host latency: 2.34195 ms (enqueue 0.042746 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.657098 ms - Host latency: 2.3019 ms (enqueue 0.0432159 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655872 ms - Host latency: 2.37424 ms (enqueue 0.0447632 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654138 ms - Host latency: 2.32872 ms (enqueue 0.043457 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654953 ms - Host latency: 2.29548 ms (enqueue 0.0422729 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.644208 ms - Host latency: 2.3665 ms (enqueue 0.042868 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653735 ms - Host latency: 2.30314 ms (enqueue 0.0439331 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.651251 ms - Host latency: 2.33605 ms (enqueue 0.0446381 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.645728 ms - Host latency: 2.33384 ms (enqueue 0.0466614 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659985 ms - Host latency: 2.32899 ms (enqueue 0.0465515 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.651764 ms - Host latency: 2.36045 ms (enqueue 0.0451294 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.65415 ms - Host latency: 2.34864 ms (enqueue 0.0510681 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653009 ms - Host latency: 2.29222 ms (enqueue 0.0492554 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649329 ms - Host latency: 2.3309 ms (enqueue 0.0470276 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655676 ms - Host latency: 2.35601 ms (enqueue 0.050592 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659857 ms - Host latency: 2.319 ms (enqueue 0.059137 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.663867 ms - Host latency: 2.38356 ms (enqueue 0.0517578 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.664996 ms - Host latency: 2.42784 ms (enqueue 0.0506775 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653113 ms - Host latency: 2.38 ms (enqueue 0.0531921 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.646765 ms - Host latency: 2.39655 ms (enqueue 0.0526062 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649207 ms - Host latency: 2.35256 ms (enqueue 0.0490234 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.65249 ms - Host latency: 2.36462 ms (enqueue 0.05 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.650342 ms - Host latency: 2.31046 ms (enqueue 0.0504578 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.652087 ms - Host latency: 2.37595 ms (enqueue 0.0547424 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649817 ms - Host latency: 2.30176 ms (enqueue 0.0563904 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.657916 ms - Host latency: 2.37327 ms (enqueue 0.0410828 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653113 ms - Host latency: 2.36273 ms (enqueue 0.0408569 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.657916 ms - Host latency: 2.3584 ms (enqueue 0.0414429 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656384 ms - Host latency: 2.42576 ms (enqueue 0.160278 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656989 ms - Host latency: 2.38662 ms (enqueue 0.223688 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.651697 ms - Host latency: 2.34792 ms (enqueue 0.0981628 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.652399 ms - Host latency: 2.3749 ms (enqueue 0.0759888 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.640106 ms - Host latency: 2.32752 ms (enqueue 0.0772888 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.651575 ms - Host latency: 2.36267 ms (enqueue 0.0708801 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653925 ms - Host latency: 2.3219 ms (enqueue 0.0651794 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654767 ms - Host latency: 2.37043 ms (enqueue 0.0605469 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654547 ms - Host latency: 2.3484 ms (enqueue 0.0643555 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655261 ms - Host latency: 2.33651 ms (enqueue 0.0559143 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659552 ms - Host latency: 2.37746 ms (enqueue 0.0533081 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.658649 ms - Host latency: 2.38893 ms (enqueue 0.0521301 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654132 ms - Host latency: 2.33895 ms (enqueue 0.0590149 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659973 ms - Host latency: 2.35649 ms (enqueue 0.0618713 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656403 ms - Host latency: 2.39648 ms (enqueue 0.05354 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654767 ms - Host latency: 2.32101 ms (enqueue 0.0500977 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.673682 ms - Host latency: 2.347 ms (enqueue 0.0558044 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.673065 ms - Host latency: 2.32711 ms (enqueue 0.0546143 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684235 ms - Host latency: 2.34447 ms (enqueue 0.0553101 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.681915 ms - Host latency: 2.33032 ms (enqueue 0.0566223 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.682703 ms - Host latency: 2.34463 ms (enqueue 0.0409851 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.690576 ms - Host latency: 2.36401 ms (enqueue 0.0426636 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.689758 ms - Host latency: 2.31091 ms (enqueue 0.0499878 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684961 ms - Host latency: 2.34377 ms (enqueue 0.0523682 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684827 ms - Host latency: 2.39846 ms (enqueue 0.0525635 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.693469 ms - Host latency: 2.33909 ms (enqueue 0.0523926 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.690271 ms - Host latency: 2.36361 ms (enqueue 0.0466553 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.695911 ms - Host latency: 2.34249 ms (enqueue 0.0499634 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.700098 ms - Host latency: 2.31849 ms (enqueue 0.0506714 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.694055 ms - Host latency: 2.32825 ms (enqueue 0.0515137 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.699719 ms - Host latency: 2.32748 ms (enqueue 0.0466675 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.69602 ms - Host latency: 2.32001 ms (enqueue 0.0473633 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.696643 ms - Host latency: 2.35864 ms (enqueue 0.0508301 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.693811 ms - Host latency: 2.36334 ms (enqueue 0.0508301 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.687549 ms - Host latency: 2.37216 ms (enqueue 0.0470215 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.691126 ms - Host latency: 2.34447 ms (enqueue 0.0537598 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.686816 ms - Host latency: 2.329 ms (enqueue 0.0552124 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684448 ms - Host latency: 2.3324 ms (enqueue 0.0514771 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678601 ms - Host latency: 2.40497 ms (enqueue 0.054602 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.686194 ms - Host latency: 2.3141 ms (enqueue 0.0566284 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.679956 ms - Host latency: 2.34352 ms (enqueue 0.060791 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.680225 ms - Host latency: 2.34717 ms (enqueue 0.0605591 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678516 ms - Host latency: 2.3135 ms (enqueue 0.0517822 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.677283 ms - Host latency: 2.31943 ms (enqueue 0.0580688 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.671399 ms - Host latency: 2.375 ms (enqueue 0.0540771 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.674817 ms - Host latency: 2.28251 ms (enqueue 0.0549805 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678723 ms - Host latency: 2.33047 ms (enqueue 0.0526001 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.680957 ms - Host latency: 2.32639 ms (enqueue 0.0468994 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.677795 ms - Host latency: 2.32073 ms (enqueue 0.0549561 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.668201 ms - Host latency: 2.33086 ms (enqueue 0.0596436 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.669897 ms - Host latency: 2.32616 ms (enqueue 0.0561523 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.66947 ms - Host latency: 2.34034 ms (enqueue 0.053772 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.66897 ms - Host latency: 2.30352 ms (enqueue 0.0526001 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659082 ms - Host latency: 2.33846 ms (enqueue 0.0512939 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655261 ms - Host latency: 2.32303 ms (enqueue 0.0571289 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.663562 ms - Host latency: 2.35765 ms (enqueue 0.0700439 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.661719 ms - Host latency: 2.32146 ms (enqueue 0.0578247 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.652991 ms - Host latency: 2.30693 ms (enqueue 0.0585205 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.660596 ms - Host latency: 2.31848 ms (enqueue 0.0542847 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.662952 ms - Host latency: 2.32491 ms (enqueue 0.0674438 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.666528 ms - Host latency: 2.33767 ms (enqueue 0.0703857 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.66604 ms - Host latency: 2.30562 ms (enqueue 0.0706421 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.664453 ms - Host latency: 2.31793 ms (enqueue 0.069458 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.668958 ms - Host latency: 2.34655 ms (enqueue 0.0552124 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.661731 ms - Host latency: 2.31605 ms (enqueue 0.0640137 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654346 ms - Host latency: 2.30087 ms (enqueue 0.0606812 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659876 ms - Host latency: 2.35428 ms (enqueue 0.0597412 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.661304 ms - Host latency: 2.31138 ms (enqueue 0.0557251 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659082 ms - Host latency: 2.33335 ms (enqueue 0.0584106 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655969 ms - Host latency: 2.32711 ms (enqueue 0.054248 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659058 ms - Host latency: 2.34099 ms (enqueue 0.0733765 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.657727 ms - Host latency: 2.31144 ms (enqueue 0.0730347 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.662512 ms - Host latency: 2.32054 ms (enqueue 0.0708862 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.669897 ms - Host latency: 2.32518 ms (enqueue 0.0663574 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.660974 ms - Host latency: 2.32834 ms (enqueue 0.0651733 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.658752 ms - Host latency: 2.36716 ms (enqueue 0.0650635 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656091 ms - Host latency: 2.30648 ms (enqueue 0.0627075 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653333 ms - Host latency: 2.3147 ms (enqueue 0.0596069 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.646558 ms - Host latency: 2.34352 ms (enqueue 0.0612671 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.65199 ms - Host latency: 2.3183 ms (enqueue 0.0656372 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.6453 ms - Host latency: 2.3511 ms (enqueue 0.0719727 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653088 ms - Host latency: 2.34622 ms (enqueue 0.0657471 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.65033 ms - Host latency: 2.27391 ms (enqueue 0.0726563 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.653027 ms - Host latency: 2.33905 ms (enqueue 0.0529297 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654138 ms - Host latency: 2.3578 ms (enqueue 0.0553589 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656665 ms - Host latency: 2.35183 ms (enqueue 0.0649292 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.666125 ms - Host latency: 2.30825 ms (enqueue 0.0631104 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.658264 ms - Host latency: 2.32413 ms (enqueue 0.0623047 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655872 ms - Host latency: 2.3428 ms (enqueue 0.0605469 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.664148 ms - Host latency: 2.33513 ms (enqueue 0.0635376 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.662122 ms - Host latency: 2.32264 ms (enqueue 0.0684448 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655481 ms - Host latency: 2.42319 ms (enqueue 0.0690918 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.667151 ms - Host latency: 2.38287 ms (enqueue 0.061377 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.671545 ms - Host latency: 2.32413 ms (enqueue 0.0650635 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.666846 ms - Host latency: 2.37577 ms (enqueue 0.0592773 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.667932 ms - Host latency: 2.29324 ms (enqueue 0.0540649 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.676562 ms - Host latency: 2.38922 ms (enqueue 0.0556885 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684253 ms - Host latency: 2.37704 ms (enqueue 0.0584595 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678271 ms - Host latency: 2.32874 ms (enqueue 0.0743164 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.666101 ms - Host latency: 2.38015 ms (enqueue 0.10946 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.679639 ms - Host latency: 2.35718 ms (enqueue 0.0747437 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.683325 ms - Host latency: 2.36068 ms (enqueue 0.071936 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.681665 ms - Host latency: 2.36914 ms (enqueue 0.065918 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.681592 ms - Host latency: 2.34653 ms (enqueue 0.0696777 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684302 ms - Host latency: 2.35693 ms (enqueue 0.0641357 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.683911 ms - Host latency: 2.36118 ms (enqueue 0.0723145 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.681543 ms - Host latency: 2.31687 ms (enqueue 0.0677002 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.682324 ms - Host latency: 2.35205 ms (enqueue 0.0705078 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.692651 ms - Host latency: 2.35925 ms (enqueue 0.0743652 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.689209 ms - Host latency: 2.38408 ms (enqueue 0.0821045 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.692529 ms - Host latency: 2.33081 ms (enqueue 0.0570801 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.692896 ms - Host latency: 2.3374 ms (enqueue 0.0567139 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.689893 ms - Host latency: 2.30354 ms (enqueue 0.0599609 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.695117 ms - Host latency: 2.3707 ms (enqueue 0.0561768 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.694922 ms - Host latency: 2.37454 ms (enqueue 0.0552002 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.694458 ms - Host latency: 2.35862 ms (enqueue 0.0557617 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.688281 ms - Host latency: 2.31951 ms (enqueue 0.0706543 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.685596 ms - Host latency: 2.32812 ms (enqueue 0.0578369 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.6771 ms - Host latency: 2.33655 ms (enqueue 0.0603516 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.683691 ms - Host latency: 2.36018 ms (enqueue 0.061499 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684546 ms - Host latency: 2.32952 ms (enqueue 0.0610107 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.685376 ms - Host latency: 2.3498 ms (enqueue 0.0563721 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.710132 ms - Host latency: 2.36575 ms (enqueue 0.0581543 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.681836 ms - Host latency: 2.34109 ms (enqueue 0.0679199 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.683057 ms - Host latency: 2.33682 ms (enqueue 0.0837158 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.681152 ms - Host latency: 2.40198 ms (enqueue 0.0656982 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.686865 ms - Host latency: 2.32681 ms (enqueue 0.0559326 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.684229 ms - Host latency: 2.30867 ms (enqueue 0.0548096 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.674658 ms - Host latency: 2.36575 ms (enqueue 0.07229 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.674365 ms - Host latency: 2.36023 ms (enqueue 0.0542725 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678394 ms - Host latency: 2.38721 ms (enqueue 0.0647949 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.674194 ms - Host latency: 2.33066 ms (enqueue 0.0598145 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.669873 ms - Host latency: 2.36455 ms (enqueue 0.0570313 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.669702 ms - Host latency: 2.35049 ms (enqueue 0.0532959 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.654761 ms - Host latency: 2.37449 ms (enqueue 0.0522949 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655151 ms - Host latency: 2.37612 ms (enqueue 0.0645996 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649951 ms - Host latency: 2.33035 ms (enqueue 0.0539307 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656421 ms - Host latency: 2.39204 ms (enqueue 0.0676514 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.661963 ms - Host latency: 2.32585 ms (enqueue 0.0644531 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.662036 ms - Host latency: 2.36431 ms (enqueue 0.0622559 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.663452 ms - Host latency: 2.3678 ms (enqueue 0.0652588 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.662744 ms - Host latency: 2.35825 ms (enqueue 0.0674316 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.663794 ms - Host latency: 2.40925 ms (enqueue 0.0576416 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.663745 ms - Host latency: 2.36511 ms (enqueue 0.0552246 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.660449 ms - Host latency: 2.35537 ms (enqueue 0.0583496 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.657324 ms - Host latency: 2.36328 ms (enqueue 0.069751 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.65022 ms - Host latency: 2.33279 ms (enqueue 0.085498 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.64895 ms - Host latency: 2.3948 ms (enqueue 0.0845703 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.643579 ms - Host latency: 2.34546 ms (enqueue 0.0848877 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.6521 ms - Host latency: 2.35525 ms (enqueue 0.0777588 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.647192 ms - Host latency: 2.36421 ms (enqueue 0.0764404 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.659229 ms - Host latency: 2.34827 ms (enqueue 0.0707764 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.645728 ms - Host latency: 2.34729 ms (enqueue 0.0724365 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649658 ms - Host latency: 2.39521 ms (enqueue 0.0701904 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.665015 ms - Host latency: 2.37544 ms (enqueue 0.0645752 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655957 ms - Host latency: 2.33601 ms (enqueue 0.0727783 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.649634 ms - Host latency: 2.33435 ms (enqueue 0.077832 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.660889 ms - Host latency: 2.34546 ms (enqueue 0.0703613 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.661499 ms - Host latency: 2.34873 ms (enqueue 0.0817871 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.664575 ms - Host latency: 2.35046 ms (enqueue 0.0698486 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.66521 ms - Host latency: 2.38721 ms (enqueue 0.0719238 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.660889 ms - Host latency: 2.39478 ms (enqueue 0.0699219 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.65188 ms - Host latency: 2.32659 ms (enqueue 0.0782471 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.656812 ms - Host latency: 2.39031 ms (enqueue 0.0681396 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.652856 ms - Host latency: 2.31814 ms (enqueue 0.0640625 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655981 ms - Host latency: 2.38513 ms (enqueue 0.0708984 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.655518 ms - Host latency: 2.32979 ms (enqueue 0.0707031 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.664478 ms - Host latency: 2.3981 ms (enqueue 0.0642334 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.660718 ms - Host latency: 2.3519 ms (enqueue 0.0658203 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.663867 ms - Host latency: 2.36804 ms (enqueue 0.0625732 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.658936 ms - Host latency: 2.34392 ms (enqueue 0.0636719 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.669678 ms - Host latency: 2.43662 ms (enqueue 0.0676758 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.67439 ms - Host latency: 2.4011 ms (enqueue 0.0744141 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.672412 ms - Host latency: 2.37029 ms (enqueue 0.0691406 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.673901 ms - Host latency: 2.34819 ms (enqueue 0.0596191 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.674829 ms - Host latency: 2.31963 ms (enqueue 0.0558594 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.671558 ms - Host latency: 2.35244 ms (enqueue 0.0662598 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.673804 ms - Host latency: 2.34971 ms (enqueue 0.0642334 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.667773 ms - Host latency: 2.36824 ms (enqueue 0.0634766 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.666504 ms - Host latency: 2.37478 ms (enqueue 0.0645752 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.673169 ms - Host latency: 2.36943 ms (enqueue 0.064502 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.681934 ms - Host latency: 2.40833 ms (enqueue 0.0567139 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.680957 ms - Host latency: 2.40593 ms (enqueue 0.0970703 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.671509 ms - Host latency: 2.37131 ms (enqueue 0.0770996 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678101 ms - Host latency: 2.37234 ms (enqueue 0.0557129 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.680444 ms - Host latency: 2.41641 ms (enqueue 0.0548828 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.675024 ms - Host latency: 2.33218 ms (enqueue 0.0554932 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678296 ms - Host latency: 2.38508 ms (enqueue 0.0567383 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.680469 ms - Host latency: 2.37139 ms (enqueue 0.0556641 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.680176 ms - Host latency: 2.32327 ms (enqueue 0.05979 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.682495 ms - Host latency: 2.3491 ms (enqueue 0.0588867 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678882 ms - Host latency: 2.29329 ms (enqueue 0.0641113 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.677222 ms - Host latency: 2.32693 ms (enqueue 0.0641113 ms)\n",
      "[10/07/2025-13:49:22] [I] Average on 10 runs - GPU latency: 0.678003 ms - Host latency: 2.38105 ms (enqueue 0.0612549 ms)\n",
      "[10/07/2025-13:49:22] [I] \n",
      "[10/07/2025-13:49:22] [I] === Performance summary ===\n",
      "[10/07/2025-13:49:22] [I] Throughput: 800.38 qps\n",
      "[10/07/2025-13:49:22] [I] Latency: min = 2.00659 ms, max = 2.79211 ms, mean = 2.34724 ms, median = 2.35516 ms, percentile(90%) = 2.51343 ms, percentile(95%) = 2.55273 ms, percentile(99%) = 2.62463 ms\n",
      "[10/07/2025-13:49:22] [I] Enqueue Time: min = 0.0355835 ms, max = 0.623352 ms, mean = 0.0610726 ms, median = 0.0568848 ms, percentile(90%) = 0.0808105 ms, percentile(95%) = 0.0900879 ms, percentile(99%) = 0.120117 ms\n",
      "[10/07/2025-13:49:22] [I] H2D Latency: min = 0.324707 ms, max = 0.99408 ms, mean = 0.565795 ms, median = 0.564331 ms, percentile(90%) = 0.758057 ms, percentile(95%) = 0.795654 ms, percentile(99%) = 0.858093 ms\n",
      "[10/07/2025-13:49:22] [I] GPU Compute Time: min = 0.624634 ms, max = 0.953369 ms, mean = 0.667235 ms, median = 0.665588 ms, percentile(90%) = 0.689209 ms, percentile(95%) = 0.695312 ms, percentile(99%) = 0.703613 ms\n",
      "[10/07/2025-13:49:22] [I] D2H Latency: min = 0.937744 ms, max = 1.42749 ms, mean = 1.11422 ms, median = 1.11322 ms, percentile(90%) = 1.22083 ms, percentile(95%) = 1.25192 ms, percentile(99%) = 1.31519 ms\n",
      "[10/07/2025-13:49:22] [I] Total Host Walltime: 3.00357 s\n",
      "[10/07/2025-13:49:22] [I] Total GPU Compute Time: 1.60403 s\n",
      "[10/07/2025-13:49:22] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[10/07/2025-13:49:22] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v100900] [b34] # trtexec --onnx=./model.onnx --staticPlugins=/usr/local/lib/python3.12/dist-packages/bluewalm/operators/tensorrt/libbluewalmPlugin.so --loadInputs=input:./input.dat --inputIOFormats=bf16:chw,bf16:chw,bf16:chw --outputIOFormats=bf16:chw,bf16:chw,bf16:chw --bf16 --minShapes=input:4x2048x512,k_cache:4x512x0,v_cache:4x512x0 --optShapes=input:4x2048x512,k_cache:4x512x0,v_cache:4x512x0 --maxShapes=input:4x2048x512,k_cache:4x512x0,v_cache:4x512x0 --builderOptimizationLevel=5 --maxAuxStreams=2 --saveEngine=./model.engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025-13:49:22] [W] * Throughput may be bound by device-to-host transfers for the outputs rather than GPU Compute and the GPU may be under-utilized.\n",
      "[10/07/2025-13:49:22] [W]   Add --noDataTransfers flag to disable data transfers.\n",
      "[10/07/2025-13:49:22] [W] * GPU compute time is unstable, with coefficient of variance = 2.55184%.\n",
      "[10/07/2025-13:49:22] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n"
     ]
    }
   ],
   "source": [
    "# create the model \n",
    "softplus_attention = AttentionLayerSP(dim, core_dim).cuda().bfloat16()\n",
    "# create a sample input \n",
    "x = get_input(bsz, seqlen, dim, 'cuda', torch.bfloat16)\n",
    "k_cache, v_cache = get_sp_cache(bsz, cache_len, core_dim, 'cuda', torch.bfloat16)\n",
    "sample = (x, k_cache, v_cache)\n",
    "\n",
    "# warmup and recompilation of torchscript models may take too long to run.... \n",
    "# especially for large models\n",
    "# here we just turn off graph optimizations to deal with that\n",
    "with torch.no_grad() and torch.jit.optimized_execution(False):\n",
    "    # ts export\n",
    "    softplus_attention_traced = torch.jit.trace(softplus_attention, sample, check_trace=False)\n",
    "    \n",
    "    # onnx export\n",
    "    onnx_model_path = \"model.onnx\"\n",
    "    torch.onnx.export(softplus_attention_traced, sample, onnx_model_path, verbose=False, \n",
    "                      opset_version=18, export_params=True, \n",
    "                      keep_initializers_as_inputs=False, \n",
    "                      custom_opsets={\"trt.plugins\" : 1}, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['input', 'k_cache', 'v_cache'], \n",
    "                      output_names=['output', 'updated_k_cache', 'updated_v_cache'], \n",
    "                      dynamic_axes={'input' : {0 : 'batch_size', 1 : 'query_len'}, \n",
    "                                    'k_cache' : {0 : 'batch_size', 2 : 'cache_len'}, \n",
    "                                    'v_cache' : {0 : 'batch_size', 2 : 'cache_len'}, \n",
    "                                    'output' : {0 : 'batch_size', 1 : 'query_len'}, \n",
    "                                    'updated_k_cache' : {0 : 'batch_size', 2 : 'updated_cache_len'}, \n",
    "                                    'updated_v_cache' : {0 : 'batch_size', 2 : 'updated_cache_len'}})\n",
    "\n",
    "# tensorrt export\n",
    "package_path = os.path.dirname(os.path.realpath(bluewalm.__file__))\n",
    "plugin_path = os.path.join(package_path, 'operators', 'tensorrt', 'libbluewalmPlugin.so')\n",
    "\n",
    "export_bf16_inputs(sample)\n",
    "\n",
    "command = \"trtexec --onnx=./model.onnx --staticPlugins=\" + str(plugin_path)\n",
    "command += \" --loadInputs=input:./input.dat\"\n",
    "if k_cache.numel() > 0:\n",
    "    command += \",k_cache:./k_cache.dat,v_cache:./v_cache.dat\"\n",
    "command += \" --inputIOFormats=bf16:chw,bf16:chw,bf16:chw\"\n",
    "command += \" --outputIOFormats=bf16:chw,bf16:chw,bf16:chw\"\n",
    "command += \" --bf16\"\n",
    "input_shape = shape_to_str(sample[0].shape)\n",
    "k_cache_shape = shape_to_str(sample[1].shape)\n",
    "v_cache_shape = shape_to_str(sample[2].shape)\n",
    "shapes = \"input:\" + input_shape + \",k_cache:\" + k_cache_shape + \",v_cache:\" + v_cache_shape\n",
    "command += \" --minShapes=\" + shapes\n",
    "command += \" --optShapes=\" + shapes\n",
    "command += \" --maxShapes=\" + shapes\n",
    "command += \" --builderOptimizationLevel=5\"\n",
    "command += \" --maxAuxStreams=2\"  # the larger `maxAuxStreams` is, the more memory the engine needs! \n",
    "#command += \" --memPoolSize=workspace:16384\"\n",
    "command += \" --saveEngine=./model.engine\"\n",
    "# command += \" --skipInference\"\n",
    "execute(command)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb85d4-d658-44ba-9f79-d4ea7eb5cfc2",
   "metadata": {},
   "source": [
    "<br>\n",
    "Thus, we can see that the mean latency of the softmax attention layer is <b>53.2391 ms</b>. <br>\n",
    "Moreover, the mean latency of the softplus attention layer is <b>2.34724 ms</b>. <br><br>\n",
    "We can also see that the exact nature of the performance improvement is difficult to determine.... <br>\n",
    "Indeed, for softmax attention, latency is heavily influenced by query length, the token dimension and the number of attention heads.... <br>\n",
    "....while softplus attention scales much more nicely. It was designed to scale. <br><br>\n",
    "Furthermore, the overall impact on the entire neural network has to be studied... as that's what counts. <br>\n",
    "In any event, we can safely conclude that there seems to be a lot of improvement. <br><br>\n",
    "\n",
    "A deployment script that deploys an entire neural network can be found in the bluewalm github repository. <br><br>\n",
    "That's it, this concludes the notebook! <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dec0e0-aeef-49af-8f99-2515a7290212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
